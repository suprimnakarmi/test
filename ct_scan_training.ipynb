{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Required python libraries         \n",
    "import numpy as np         \n",
    "import os                  \n",
    "from random import shuffle\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import glob\n",
    "from tqdm import tqdm \n",
    "import time\n",
    "import pickle\n",
    "from tqdm import tqdm\n",
    "from statistics import mode\n",
    "\n",
    "\n",
    "# OpenCV and scikit-learn\n",
    "from sklearn.utils import resample\n",
    "from sklearn.metrics.pairwise import euclidean_distances\n",
    "from sklearn.metrics.pairwise import manhattan_distances, cosine_distances\n",
    "from sklearn.metrics import silhouette_score\n",
    "# from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.metrics import auc\n",
    "from sklearn.metrics import davies_bouldin_score\n",
    "import cv2 \n",
    "\n",
    "# Pandas \n",
    "# import pandas as pd\n",
    "\n",
    "# Tensorflow\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "from tensorflow.keras.applications.densenet import DenseNet169\n",
    "from tensorflow.keras.applications.vgg16 import VGG16\n",
    "from tensorflow.keras.applications.resnet import ResNet101 \n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "select_model = int(input(\"Enter the number for: \\n 1) VGGNET16 \\n 2) Resnet101  \\n 3) Densenet161 \"))\n",
    "\n",
    "select_distance = int(input(\"Enter the number for: \\n 1) Euclidean  \\n 2) Manhattan \\n 3) Cosine\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data= pd.read_csv(\"./ct_scan_dataset/all_image_paths.txt\", sep=\" \", header=None)\n",
    "data.columns = ['filename', 'label', 'xmin','ymin','xmax','ymax']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nor_img, pne_img, cov_img = [],[],[]\n",
    "nor_id, pne_id, cov_id = [], [], []\n",
    "id_count = 0\n",
    "for i in range(len(data)):\n",
    "\n",
    "    if data[\"label\"][i]== 0:\n",
    "        # nor_img.append(data[\"filename\"][i])\n",
    "        nor_id.append(i)\n",
    "    elif data[\"label\"][i] == 1:\n",
    "        # pne_img.append(data[\"filename\"][i])\n",
    "        pne_id.append(i)\n",
    "    else:\n",
    "        # cov_img.append(data[\"filename\"][i])\n",
    "        cov_id.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(42)\n",
    "shuffle(nor_id)\n",
    "# random.seed(42)\n",
    "shuffle(pne_id)\n",
    "# random.seed(42)\n",
    "shuffle(cov_id)\n",
    "\n",
    "cov_select = cov_id[:5000]\n",
    "nor_select = nor_id[:2500]\n",
    "pne_select = pne_id[:2500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# count = 0     # Count to record the ids of files. Each file has a unique ID.\n",
    "img_size = 224\n",
    "def get_dataset(files,cov_select, nor_select, pne_select):  \n",
    "  count=0      \n",
    "  dataset=[]  # List to hold all the dataset. Each element is a dictionary\n",
    "  # image_only=[]\n",
    "  for i in tqdm(cov_select + nor_select + pne_select):  # Loop over each file location\n",
    "    data_dict = {}  \n",
    "    data_dict[\"id\"] = count\n",
    "    data_dict[\"filepath\"] = os.path.join(\"./ct_scan_dataset/3A_images/\",files[\"filename\"][i])\n",
    "    img=cv2.imread(os.path.join(\"./ct_scan_dataset/3A_images/\",files[\"filename\"][i]))\n",
    "\n",
    "    x_min, y_min, x_max, y_max = files[\"xmin\"][i], files[\"ymin\"][i], files[\"xmax\"][i], files[\"ymax\"][i]\n",
    "    cropped_img = img[y_min:y_max,x_min:x_max,:]\n",
    "    \n",
    "    img_resized = cv2.resize(cropped_img,(img_size,img_size))\n",
    "    data_dict[\"image\"]=img_resized\n",
    "    # image_only.append(img_resized)\n",
    "    if files[\"label\"][i]== 0 or files[\"label\"][i]==1:\n",
    "      data_dict[\"label\"]= 0\n",
    "    else:\n",
    "      data_dict[\"label\"]=1\n",
    "    count +=1\n",
    "    dataset.append(data_dict)\n",
    "  return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = get_dataset(data, cov_select, nor_select, pne_select)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_only = []\n",
    "for data in dataset:\n",
    "    image_only.append(data[\"image\"])\n",
    "\n",
    "image_only = np.array(image_only)\n",
    "batch_size=2000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_datagen = ImageDataGenerator()\n",
    "batch_img= img_datagen.flow(image_only, batch_size=batch_size, shuffle = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Method to return the pretrained models\n",
    "def all_models(img_size, model_sel):\n",
    " \n",
    "  if model_sel == 1:\n",
    "    vgg_pre_t = VGG16(input_shape = (img_size, img_size, 3),include_top = False, weights ='imagenet')\n",
    "    return vgg_pre_t, 25088\n",
    "\n",
    "  elif model_sel==2:\n",
    "    resnet_pre_t= ResNet101(input_shape = (img_size, img_size, 3),include_top=False, weights='imagenet')\n",
    "    return resnet_pre_t, 100352\n",
    "\n",
    "  elif model_sel==3:\n",
    "    densenet169_pre_t = DenseNet169(input_shape = (img_size, img_size, 3),include_top = False, weights ='imagenet' )\n",
    "    return densenet169_pre_t, 81536\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_fea = []\n",
    "model,feature_size= all_models(img_size, select_model)\n",
    "for data in tqdm(range(len(batch_img))):\n",
    "  try:\n",
    "    features = model.predict(batch_img[data]).flatten().reshape(batch_size,feature_size)\n",
    "  except:\n",
    "    img_len=len(batch_img[data])\n",
    "    features = model.predict(batch_img[data]).flatten().reshape(img_len,feature_size)\n",
    "  all_fea.extend(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(dataset)):\n",
    "  dataset[i]['image']= all_fea[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(42)\n",
    "shuffle(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def correct_mispredictions(query, fea_label,train_label, train_id, ind_data, decision,data_frame_1, count):\n",
    "    if mode(decision) != query[\"label\"]:\n",
    "        count +=1 \n",
    "        data_frame_1[\"Image name\"].append(query[\"filepath\"].split(\"/\")[-1])\n",
    "        data_frame_1[\"Mistake ID\"].append(query['id'])\n",
    "        data_frame_1[\"Original label\"].append(query['label'])\n",
    "        data_frame_1[\"Predicted label\"].append(mode(decision))\n",
    "        data_frame_1[\"Mistake index\"].append(ind_data)\n",
    "        fea_label[query['label']].append(query[\"image\"])\n",
    "        train_label[query['label']].append(query[\"label\"])\n",
    "        train_id[query['label']].append(query['id'])\n",
    "    else:\n",
    "        fea_label[query['label']].append(query[\"image\"])\n",
    "        train_label[query['label']].append(query[\"label\"])\n",
    "        train_id[query['label']].append(query['id'])\n",
    "    return count,data_frame_1,fea_label,train_label,train_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def distance1(query, fea_label, select_distance, id_pred, label_pred, n_neighbours,count,train_label, train_id, ind_data, data_frame_1,supervised_data): # Query is the raw dictionary (from pickle file) // fea_label is dictionary of {0: [], 1:[]} (distance) // select distance is int\n",
    "  exp_query = np.expand_dims(query['image'], axis=0)\n",
    "  pos_tup, neg_tup = [], []\n",
    "  \n",
    "  if select_distance==1: # Euclidean distance\n",
    "    \n",
    "    neg_dist = np.linalg.norm(query['image']-fea_label[0], axis=1)  # Calculating the Euclidean distance using numpy (axis=1) to calculate all at ones   \n",
    "    pos_dist = np.linalg.norm(query['image']-fea_label[1],axis=1)\n",
    "  \n",
    "  elif select_distance==2: # Manhattan distance\n",
    "     neg_dist = np.squeeze(manhattan_distances(fea_label[0],exp_query))  # convert (1,n) to (,n)\n",
    "     pos_dist=np.squeeze(manhattan_distances(fea_label[1],exp_query))\n",
    "\n",
    "  elif select_distance==3: # Cosine distance\n",
    "    neg_dist = np.squeeze(cosine_distances(exp_query,fea_label[0]))  # convert (1,n) to (,n)\n",
    "    pos_dist=np.squeeze(cosine_distances(exp_query,fea_label[1]))\n",
    "\n",
    "  \n",
    "  for dist_single in pos_dist:\n",
    "    pos_tup.append((dist_single,1))\n",
    "\n",
    "  for dist_single in neg_dist:\n",
    "    neg_tup.append((dist_single,0))\n",
    "\n",
    "  pos_tup.extend(neg_tup)\n",
    "  tup_dist = sorted(pos_tup)[:n_neighbours]\n",
    "  \n",
    "  \n",
    "  decision = [y for (x,y) in tup_dist]\n",
    "  if supervised_data:\n",
    "    count,data_frame_1,fea_label,train_label,train_id=correct_mispredictions(query, fea_label,train_label,train_id, ind_data, decision,data_frame_1, count)\n",
    "  \n",
    "  else:\n",
    "    if decision.count(0) > decision.count(1):\n",
    "      fea_label[0].append(query[\"image\"])\n",
    "      id_pred[0].append(query[\"id\"])\n",
    "      label_pred[0].append((query['id'],decision.count(1)/n_neighbours))\n",
    "\n",
    "    else:\n",
    "      fea_label[1].append(query[\"image\"])\n",
    "      id_pred[1].append(query[\"id\"])\n",
    "      label_pred[1].append((query['id'],decision.count(1)/n_neighbours))\n",
    "  \n",
    "  return fea_label, id_pred, label_pred, data_frame_1, count, train_label, train_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classification_metrics(label_gt,id_pred):\n",
    "  TP,FP,FN,TN = 0,0,0,0\n",
    "\n",
    "  for tp in id_pred[1]:   # TP --> When correctly classified covid\n",
    "    if tp in label_gt[1]:\n",
    "      TP +=1\n",
    "\n",
    "  for tn in id_pred[0]:  # TN --> When correctly classified healthy (non-covid)\n",
    "    if tn in label_gt[0]:\n",
    "      TN +=1\n",
    "\n",
    "  for fp in id_pred[1]: # FP --> When incorrectly classified healthy (Classified healthy as covid)\n",
    "    if fp in label_gt[0]:\n",
    "      FP +=1\n",
    "\n",
    "  for fn in id_pred[0]: # FN --> When missed covid classification (Covid cases missed)\n",
    "    if fn in label_gt[1]:\n",
    "      FN +=1\n",
    "\n",
    "  accuracy= (TP+TN)/(TP+TN+FP+FN)\n",
    "  specificity = TN/(TN+FP)\n",
    "  sensitivity = (TP)/(TP+FN)\n",
    "  # f1_score = (2*precision*recall)/(precision + recall)\n",
    "  \n",
    "  print(\"TP: \", TP)\n",
    "  print(\"FP: \", FP)\n",
    "  print(\"FN: \", FN)\n",
    "  print(\"TN: \", TN)\n",
    "\n",
    "  return accuracy, specificity, sensitivity,TP,TN,FP,FN\n",
    "\n",
    "def roc_auc_curve(label_gt,label_pred):\n",
    "  gt_labels= sorted(label_gt[0]+ label_gt[1])  # Contains (id,labels) tuple of binary class \n",
    "  pred_labels = sorted(label_pred[0]+label_pred[1]) # Contains (id,labels) tuple of binary class --> sorted to match each element in gt_labels and pred_labels\n",
    "  y_test = [y for (x,y) in gt_labels]   # Get only the labels\n",
    "  y_scores = [y for (x,y) in pred_labels]\n",
    "  fpr, tpr, threshold = roc_curve(y_test, y_scores)\n",
    "  roc_auc = auc(fpr, tpr)\n",
    "  return roc_auc\n",
    "\n",
    "def cluster_metrics(fea_label,train_label,id_pred):\n",
    "  print(\"Calculating Dunn's index...\")\n",
    "  intra_dist1 = euclidean_distances(fea_label[0]).max()\n",
    "  intra_dist2 = euclidean_distances(fea_label[1]).max()\n",
    "  inter_dist = euclidean_distances(fea_label[0],fea_label[1]).min()\n",
    "\n",
    "  if intra_dist1>intra_dist2:\n",
    "    max_intra_dist= intra_dist1  \n",
    "  else:\n",
    "    max_intra_dist = intra_dist2 \n",
    "\n",
    "  Dunn_index = inter_dist/max_intra_dist\n",
    "\n",
    "  print(\"Calculating Davies Bouldin index...\")\n",
    "\n",
    "  # Davies Bouldin and Silhouette score from sklearn library.\n",
    "  class_0 =np.concatenate((np.zeros(shape=(len(train_label[0])),dtype=int),np.zeros(shape=(len(id_pred[0])),dtype=int)))\n",
    "  class_1 = np.concatenate((np.ones(shape=(len(train_label[1])),dtype=int),np.ones(shape=(len(id_pred[1])),dtype=int)))\n",
    "  class_all = np.concatenate((class_0,class_1))\n",
    "  feature_all = np.concatenate((fea_label[0],fea_label[1]))\n",
    "\n",
    "  davies_bouldin_index = davies_bouldin_score(feature_all,class_all)\n",
    "  silhouette_index = silhouette_score(feature_all,class_all)\n",
    "\n",
    "  print(\"davies: \", davies_bouldin_index)\n",
    "  print(\"silhouette_sklearn: \", silhouette_index)\n",
    "  \n",
    "  return Dunn_index,davies_bouldin_index, silhouette_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "labeled_size = [200,400,800,1550]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_loader(dataset,n): # Method to return three sets of labeled dataset for experiment\n",
    "  labeled_data, unlabeled_data = [], [] \n",
    "\n",
    "  l_data = dataset[:n]    # First dataset // labeled\n",
    "  ul_data = dataset[n:]   # First dataset // unlabeled\n",
    "  labeled_data.append(l_data)\n",
    "  unlabeled_data.append(ul_data)\n",
    "\n",
    "  l_data = dataset[3200:3200+n]    # second dataset // labeled\n",
    "  ul_data = dataset[:3200]+dataset[3200+n:]\n",
    "  labeled_data.append(l_data)\n",
    "  unlabeled_data.append(ul_data)\n",
    "\n",
    "  l_data = dataset[6400:6400+n]     # Third dataset // labeled\n",
    "  ul_data = dataset[:6400]+dataset[6400+n:]\n",
    "  labeled_data.append(l_data)\n",
    "  unlabeled_data.append(ul_data)\n",
    "  return labeled_data, unlabeled_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_neighbours=31\n",
    "data_frame = {\"Labeled data\": [],\n",
    "              \"Dataset\": [],\n",
    "              \"Accuracy\": [],\n",
    "              \"Specificity\": [],\n",
    "              \"Sensitivity\": [],\n",
    "              \"AUC\":[],\n",
    "              \"Dunn index\": [],\n",
    "              \"Davies Bouldin\": [],\n",
    "              \"Silhouette index\":[],\n",
    "              \"TP\":[],\n",
    "              \"TN\":[],\n",
    "              \"FP\":[],\n",
    "              \"FN\":[],\n",
    "              \"pos_labeled_img\":[],\n",
    "              \"neg_labeled_img\":[],\n",
    "              \"corrected_count\":[]\n",
    "    \n",
    "}\n",
    "\n",
    "\n",
    "for size in labeled_size:\n",
    "  labeled_data, unlabeled_data = data_loader(dataset, size)\n",
    "  print(f\"labeled data length {len(labeled_data)}\")\n",
    "  print(f\"Unlabeled data length {len(unlabeled_data)}\")\n",
    "  select=0         # To select the dataset out of three sets ==> three sets: [d11, d12, d13] ==> eg: [200,200,200]\n",
    "  \n",
    "  while(select < 3):\n",
    "    data_frame_1 = {  \"Image name\": [],\n",
    "                  \"Mistake index\": [],\n",
    "                  \"Mistake ID\": [],\n",
    "                  \"Original label\": [],\n",
    "                  \"Predicted label\": []\n",
    "                  \n",
    "}\n",
    "    pos_img,neg_img=0,0\n",
    "    # mis_predict_id = {0: [],    \n",
    "    # 1 :[]}\n",
    "\n",
    "    label_gt = {0: [],    \n",
    "    1 :[]}    \n",
    "                        # Collect the ground truth (label) of all the predicting images\n",
    "    train_label = {0: [],    \n",
    "    1 :[]}    \n",
    "\n",
    "    label_pred = {0: [],\n",
    "    1 :[]}               # Collect the predicted label for all the images\n",
    "\n",
    "    id_gt = {0: [], \n",
    "         1: [] }         # Collect the ground truth (id) of all the predicting images\n",
    "\n",
    "    id_pred = {0: [],\n",
    "           1: []}        # Collect the predicted id for all the images \n",
    "\n",
    "    fea_label = {0: [],\n",
    "           1: []}\n",
    "\n",
    "    train_id ={0: [],\n",
    "         1:[]}\n",
    "    \n",
    "    training_data, supervised_data = labeled_data[select][:200], labeled_data[select][200:]\n",
    "    \n",
    "    for data in training_data:\n",
    "\n",
    "      if data[\"label\"] == 1:\n",
    "        fea_label[1].append(data['image'])\n",
    "        train_id[1].append(data['id'])\n",
    "        train_label[1].append((data['id'],data['label']))\n",
    "        pos_img +=1\n",
    "\n",
    "      else:\n",
    "        fea_label[0].append(data['image'])\n",
    "        train_id[0].append(data['id'])\n",
    "        train_label[0].append((data['id'],data['label']))\n",
    "        neg_img +=1\n",
    "    \n",
    "    print(f\"Feature length neg: {neg_img}\")\n",
    "    print(f\"Feature length pos: {pos_img} \")  \n",
    "\n",
    "    # supervised_data= True\n",
    "    count,ind_data=0,200\n",
    "    for data in supervised_data:\n",
    "      fea_label, id_pred, label_pred, data_frame_1, count, train_label, train_id=distance1(data,fea_label,select_distance,id_pred,label_pred,n_neighbours, count, train_label, train_id, ind_data, data_frame_1 ,supervised_data=True)\n",
    "      \n",
    "      ind_data +=1\n",
    "    data_f_1 = pd.DataFrame.from_dict(data_frame_1)\n",
    "    data_f_1.to_csv(f\"./test/densenet121_manhattan_mistake_{size}_{select}.csv\",index=False)\n",
    "      \n",
    "    # # supervised_data = False\n",
    "    for data in tqdm(unlabeled_data[select]):\n",
    "      if data[\"label\"]==1:\n",
    "        id_gt[1].append(data['id'])\n",
    "        label_gt[1].append((data['id'],data['label']))\n",
    "      \n",
    "      else:\n",
    "        id_gt[0].append(data['id'])\n",
    "        label_gt[0].append((data['id'],data['label']))\n",
    "\n",
    "      fea_label,id_pred,label_pred,_,_,_,_ = distance1(data,fea_label,select_distance,id_pred,label_pred,n_neighbours,count=None,train_label=None, train_id=None, ind_data=None, data_frame_1=None, supervised_data=False)  \n",
    "    accuracy, specificity, sensitivity,TP,TN,FP,FN= classification_metrics(id_gt,id_pred)\n",
    "    dunn_index, davies_bouldin_index, silhouette_index = cluster_metrics(fea_label,train_label,id_pred)\n",
    "    cl_auc = roc_auc_curve(label_gt,label_pred)\n",
    "    data_frame[\"Labeled data\"].append(size)\n",
    "    data_frame[\"Dataset\"].append(f\"d_{select}\")\n",
    "    data_frame[\"Accuracy\"].append(accuracy)\n",
    "    data_frame[\"Specificity\"].append(specificity)\n",
    "    data_frame[\"Sensitivity\"].append(sensitivity)\n",
    "    data_frame[\"AUC\"].append(cl_auc)\n",
    "    data_frame[\"Dunn index\"].append(dunn_index)\n",
    "    data_frame[\"Davies Bouldin\"].append(davies_bouldin_index)\n",
    "    data_frame[\"Silhouette index\"].append(silhouette_index)\n",
    "    data_frame[\"TP\"].append(TP)\n",
    "    data_frame[\"TN\"].append(TN)\n",
    "    data_frame[\"FP\"].append(FP)\n",
    "    data_frame[\"FN\"].append(FN)\n",
    "    data_frame[\"pos_labeled_img\"].append(pos_img)\n",
    "    data_frame[\"neg_labeled_img\"].append(neg_img)\n",
    "    data_frame[\"corrected_count\"].append(count)\n",
    "\n",
    "    print(f\"Labeled image: {size} \\t Dataset: d_{select} \\t Accuracy: {accuracy} \\t Specificity: {specificity} \\t Sensitivity: {sensitivity} \\t Dunn index: {dunn_index}  \\t Davies Bouldin: {davies_bouldin_index} \\t Silhouette index: {silhouette_index} \\t AUC: {cl_auc} \\t Corrected count: {count}\")\n",
    "    select +=1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if select_model==1:\n",
    "    s_model= 'vgg16'\n",
    "elif select_model==2:\n",
    "    s_model= 'resnet101'\n",
    "elif select_model==3:\n",
    "    s_model='densenet169'\n",
    "\n",
    "if select_distance==1:\n",
    "    s_distance='euclidean'\n",
    "elif select_distance==2:\n",
    "    s_distance='manhattan'\n",
    "elif select_distance==3:\n",
    "    s_distance='cosine'\n",
    "data_f=pd.DataFrame.from_dict(data_frame)\n",
    "data_f.to_csv(f\"./test/{s_model}_{s_distance}_dist.csv\",index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('tensorflow')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13 | packaged by conda-forge | (default, Mar 25 2022, 06:05:16) \n[Clang 12.0.1 ]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "4bd624a0593993fe43ac4046b27b898fb2ef75c21c08f81e89e64ea0f51df676"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
